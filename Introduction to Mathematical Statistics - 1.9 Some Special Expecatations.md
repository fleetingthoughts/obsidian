---
parent: "[[Introduction to Mathematical Statistics 8th ed. - Hogg, McKean, Craig]]"
tags:
  - math
  - statistics
  - robert_hogg
date_created:
---

In this section we define another function that uniquely characterizes a pdf or pmf and how this "characterizing" function can generate some well known properties of the pdf or pmf like the mean and the variance even if we don't know what the pmf or pdf could be.

We first define some of these properties called the "moments about a number" of a random variable $X$ . The expectation is actually one of these properties that can be generated by the "characterizing function" that we'll describer later in this chapter:

***Definition 1.9.1 (The Mean).*** Let $X$  be a random variable whose expectation exists.
The mean value $μ$  of $X$  is defined to be $\mu  = E(X)$ . The mean is also referred to as the first moment about 0 of a random variable, that is $E(X-0)$ . 

The other "moment about a number" of a random variable we're interested in involves the second momentand is called the variance.

**Definition 1.9.2 (Variance).*** Let $X$  be a random variable with finite mean $μ$  and
such that $E[(X −μ)^2]$  is finite. Then the variance of X is defined to be $E[(X −μ)^2]$
It is usually denoted by $σ^2$  or by $Var(X)$ .

$Var(X)$ can also be formulated a different way that is easier to compute:
$$Var(X) = E[(X-\mu)^2]$$
$$ Var(X)= E[(X^2-2X\mu+\mu^2)]$$
$$Var(X) = E(X^2) - 2\mu E(X)+ \mu^2$$
$$Var(X) = E(X^2)-\mu ^2$$ 
The variance of a random variable is not a linear operator but instead has the following property:
***Theorem 1.9.1. (Variance of linear transformations of a random variable).*** Let $X$ be a random variable with finite mean $μ$  and variance $σ^2$. Then for all constants $a$ and $b$.
$$ Var(aX+b) = a^2Var(X)$$The proof can be derived straight from the definition of the variance.

We now define a special expectation that uniquely characterizes a pdf:
***Definition 1.9.3 (Moment Generating Function).*** Let $X$ be a random variable such that for some $h > 0$, the expectation of $e^{tX}$  exists for $−h<t<h$ . The moment generating function of $X$ is defined to be the function $M(t) = E(e^{tX})$ , for $−h<t<h$ . We use the abbreviation mgf to denote the moment generating function of a random variable.

The mgf is a generalization of the mean and the variance. The mgf is of interest because it can uniquely identify distributions:

Theorem 1.9.2. Let $X$ and $Y$ be random variables with moment generating functions $M_X$  and $M_Y$, respectively, existing in open intervals about 0. Then $F_X (z) =F_Y (z)$  for all $z ∈ R$  if and only if $M_X(t) = M_Y (t)$ for all $t ∈ (−h, h)$ for some $h > 0$ .

